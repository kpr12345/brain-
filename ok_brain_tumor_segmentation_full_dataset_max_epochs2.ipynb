{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPscoE83390WBd58mXznOEZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpr12345/brain-/blob/main/ok_brain_tumor_segmentation_full_dataset_max_epochs2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iapd7jCmjBiN"
      },
      "outputs": [],
      "source": [
        "# # Brain Tumor Segmentation with U-Net + Hybrid Attention Block\n",
        "#\n",
        "# This notebook downloads the BRISC 2025 dataset from Kaggle, extracts it, defines a hybrid attention block (scSE + ECA + CBAM), integrates it into a U-Net model, and sets up data loading and a training scaffold.\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "# Step 1: Install kaggle and other dependencies\n",
        "get_ipython().system('pip install kaggle --quiet')\n",
        "get_ipython().system('pip install torch torchvision --quiet')\n",
        "get_ipython().system('pip install nibabel --quiet  # for medical image format handling')\n",
        "\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "# Step 2: Download BRISC 2025 Dataset from Kaggle\n",
        "import os\n",
        "\n",
        "# Make sure kaggle.json is in ~/.kaggle/kaggle.json\n",
        "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "# Uncomment and run once if you need to upload your kaggle.json manually\n",
        "# from google.colab import files\n",
        "# files.upload()  # Upload kaggle.json\n",
        "get_ipython().system('mv kaggle.json ~/.kaggle/')\n",
        "get_ipython().system('chmod 600 ~/.kaggle/kaggle.json')\n",
        "\n",
        "dataset_path = 'briscdataset/brisc2025'\n",
        "data_dir = './BRISC2025'\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    get_ipython().system('kaggle datasets download -d {dataset_path} --unzip -p {data_dir}')\n",
        "else:\n",
        "    print(f\"Dataset already downloaded and extracted in {data_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In[18]:\n",
        "\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Step 1: Define Hybrid Attention Block (scSE + ECA + CBAM)\n",
        "class HybridAttentionBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16, eca_kernel_size=3):\n",
        "        super(HybridAttentionBlock, self).__init__()\n",
        "        self.eca_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.eca_conv = nn.Conv1d(1, 1, kernel_size=eca_kernel_size, padding=(eca_kernel_size - 1) // 2, bias=False)\n",
        "        self.eca_sigmoid = nn.Sigmoid()\n",
        "        self.channel_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.channel_max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.channel_fc = nn.Sequential(\n",
        "            nn.Conv2d(channel, channel // reduction, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channel // reduction, channel, 1, bias=False)\n",
        "        )\n",
        "        self.channel_sigmoid = nn.Sigmoid()\n",
        "        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
        "        self.spatial_sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        eca_y = self.eca_avg_pool(x).squeeze(-1).transpose(-1, -2)\n",
        "        eca_y = self.eca_conv(eca_y)\n",
        "        eca_y = self.eca_sigmoid(eca_y).transpose(-1, -2).unsqueeze(-1)\n",
        "        avg_pool = self.channel_avg_pool(x)\n",
        "        max_pool = self.channel_max_pool(x)\n",
        "        avg_out = self.channel_fc(avg_pool)\n",
        "        max_out = self.channel_fc(max_pool)\n",
        "        ch_attn = self.channel_sigmoid(avg_out + max_out)\n",
        "        channel_attn = eca_y * ch_attn\n",
        "        x_channel = x * channel_attn.expand_as(x)\n",
        "        max_result, _ = torch.max(x_channel, dim=1, keepdim=True)\n",
        "        avg_result = torch.mean(x_channel, dim=1, keepdim=True)\n",
        "        spatial_input = torch.cat([max_result, avg_result], dim=1)\n",
        "        spatial_attn = self.spatial_sigmoid(self.spatial_conv(spatial_input))\n",
        "        out = x_channel * spatial_attn.expand_as(x_channel)\n",
        "        return out\n",
        "\n",
        "# Step 2: Define U-Net with Hybrid Attention\n",
        "class UNetWithHybridAttention(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, base_channels=64):\n",
        "        super(UNetWithHybridAttention, self).__init__()\n",
        "        self.encoder1 = self.conv_block(in_channels, base_channels)\n",
        "        self.encoder2 = self.conv_block(base_channels, base_channels * 2)\n",
        "        self.encoder3 = self.conv_block(base_channels * 2, base_channels * 4)\n",
        "        self.encoder4 = self.conv_block(base_channels * 4, base_channels * 8)\n",
        "        self.middle = self.conv_block(base_channels * 8, base_channels * 16)\n",
        "        self.decoder4 = self.deconv_block(base_channels * 16, base_channels * 8)\n",
        "        self.decoder3 = self.deconv_block(base_channels * 8, base_channels * 4)\n",
        "        self.decoder2 = self.deconv_block(base_channels * 4, base_channels * 2)\n",
        "        self.decoder1 = self.deconv_block(base_channels * 2, base_channels)\n",
        "        self.final_conv = nn.Conv2d(base_channels, out_channels, kernel_size=1)\n",
        "        self.attn1 = HybridAttentionBlock(base_channels)\n",
        "        self.attn2 = HybridAttentionBlock(base_channels * 2)\n",
        "        self.attn3 = HybridAttentionBlock(base_channels * 4)\n",
        "        self.attn4 = HybridAttentionBlock(base_channels * 8)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def deconv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def center_crop(self, tensor, target_tensor_shape):\n",
        "        _, _, tensor_h, tensor_w = tensor.size()\n",
        "        _, _, target_h, target_w = target_tensor_shape\n",
        "\n",
        "        diff_h = tensor_h - target_h\n",
        "        diff_w = tensor_w - target_w\n",
        "\n",
        "        crop_top = diff_h // 2\n",
        "        crop_left = diff_w // 2\n",
        "\n",
        "        return tensor[:, :, crop_top:crop_top + target_h, crop_left:crop_left + target_w]\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(enc1)\n",
        "        enc3 = self.encoder3(enc2)\n",
        "        enc4 = self.encoder4(enc3)\n",
        "        middle = self.middle(enc4)\n",
        "        dec4 = self.attn4(self.decoder4(middle))\n",
        "        dec3 = self.attn3(self.decoder3(self.center_crop(dec4, enc4.shape) + enc4))\n",
        "        dec2 = self.attn2(self.decoder2(self.center_crop(dec3, enc3.shape) + enc3))\n",
        "        dec1 = self.attn1(self.decoder1(self.center_crop(dec2, enc2.shape) + enc2))\n",
        "        out = self.final_conv(dec1)\n",
        "        return out\n",
        "\n",
        "# Step 3: Define Dataset\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_paths = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg'))])\n",
        "        self.mask_paths = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith(('.png', '.jpg'))])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert('L')\n",
        "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask) # Apply transform to the mask as well\n",
        "        mask = (np.array(mask) > 0).astype(np.uint8)\n",
        "        mask = torch.tensor(mask, dtype=torch.long)\n",
        "        mask = mask.squeeze(0) # Remove the channel dimension\n",
        "        return image, mask\n",
        "\n",
        "# Step 4: Training and Evaluation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = BrainTumorDataset('./BRISC2025/brisc2025/segmentation_task/train/images',\n",
        "                             './BRISC2025/brisc2025/segmentation_task/train/masks',\n",
        "                             transform=transform)\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = UNetWithHybridAttention(in_channels=1, out_channels=2).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "num_epochs = 20\n",
        "train_loss, train_acc, train_precision, train_recall, train_f1, train_mse = [], [], [], [], [], []"
      ],
      "metadata": {
        "id": "-qbmgCxbjEQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss, preds_list, targets_list = 0, [], []\n",
        "    for images, masks in dataloader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Resize masks to match the output size of the U-Net\n",
        "        masks = F.interpolate(masks.unsqueeze(1).float(), size=outputs.shape[2:], mode='nearest').squeeze(1).long()\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, dim=1).cpu().numpy().flatten()\n",
        "        targets = masks.cpu().numpy().flatten()\n",
        "        preds_list.extend(preds)\n",
        "        targets_list.extend(targets)\n",
        "    epoch_loss /= len(dataloader)\n",
        "    train_loss.append(epoch_loss)\n",
        "    train_acc.append(accuracy_score(targets_list, preds_list))\n",
        "    train_precision.append(precision_score(targets_list, preds_list))\n",
        "    train_recall.append(recall_score(targets_list, preds_list))\n",
        "    train_f1.append(f1_score(targets_list, preds_list))\n",
        "    train_mse.append(mean_squared_error(targets_list, preds_list))\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} Acc: {train_acc[-1]:.4f} F1: {train_f1[-1]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "pevcPsobjPJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Plot Results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_loss, label='Loss')\n",
        "plt.plot(train_acc, label='Accuracy')\n",
        "plt.plot(train_f1, label='F1 Score')\n",
        "plt.title('Training Performance')\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_precision, label='Precision')\n",
        "plt.plot(train_recall, label='Recall')\n",
        "plt.plot(train_mse, label='MSE')\n",
        "plt.title('Training Metrics')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tvRiqvtojWGa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}